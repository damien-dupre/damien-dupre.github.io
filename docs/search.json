[
  {
    "objectID": "posts/list/timeseries-clustering-part2.html",
    "href": "posts/list/timeseries-clustering-part2.html",
    "title": "Time series clustering with Dynamic Time Warping (Part 2)",
    "section": "",
    "text": "Like any good movie, my previous post, “Time Series Clustering with Dynamic Time Warping,” deserves a sequel. In this Part 2, I’ll examine athletes’ training plans for a marathon. Because marathons are so demanding, most runners follow a specific training plan to prepare. You can find many different plans on the web, like this one from Run Ireland.\nIn this post, I’ll attempt to cluster several simulated training plans. To do this, I will use Dynamic Time Warping combined with feature extraction techniques like seasonality decomposition, state-space models, and power spectrum analysis."
  },
  {
    "objectID": "posts/list/timeseries-clustering-part2.html#dtw-cluster-on-raw-data",
    "href": "posts/list/timeseries-clustering-part2.html#dtw-cluster-on-raw-data",
    "title": "Time series clustering with Dynamic Time Warping (Part 2)",
    "section": "DTW cluster on raw data",
    "text": "DTW cluster on raw data\nAfter preparing the data list, let’s run a simple DTW clustering on the raw data to see if we can identify our two groups.\n\nDTW model\n\nNclust &lt;- 2\ndtw_model &lt;- dtwclust::tsclust(\n  series = plan_list,\n  type = \"h\",\n  k = Nclust,\n  distance = \"dtw_basic\",\n  control = hierarchical_control(method = \"complete\"),\n  preproc = NULL,\n  # args = tsclust_args(dist = list(window.size = 5L)),\n  trace = TRUE\n)\n\n\nCalculating distance matrix...\nPerforming hierarchical clustering...\nExtracting centroids...\n\n    Elapsed time is 0.038 seconds.\n\n#\ndtw_data &lt;- ggdendro::dendro_data(dtw_model, type = \"rectangle\")\n#\nlabels_order &lt;- dtw_data$labels$label\n#\ndtw_result &lt;- data.frame(\n  label = names(plan_list),\n  cluster = factor(stats::cutree(dtw_model, k = Nclust))\n)\n#\ndtw_data[[\"labels\"]] &lt;- merge(dtw_data[[\"labels\"]], dtw_result, by = \"label\")\ndtw_result &lt;- dplyr::full_join(dtw_result, dtw_data$labels, by = c(\"label\", \"cluster\")) %&gt;%\n  dplyr::arrange(x)\n\n\n\nDTW plot\n\ncluster_box &lt;- aggregate(x ~ cluster, ggdendro::label(dtw_data), range)\ncluster_box &lt;- data.frame(cluster_box$cluster, cluster_box$x)\ncluster_threshold &lt;- mean(dtw_model$height[length(dtw_model$height) - ((Nclust - 2):(Nclust - 1))])\n#\nnumColors &lt;- length(levels(dtw_result$cluster)) # How many colors you need\ngetColors &lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)\nmyPalette &lt;- getColors(numColors)\nnames(myPalette) &lt;- levels(dtw_result$cluster) # Give every color an appropriate name\n\np1 &lt;- ggplot() +\n  geom_rect(data = cluster_box, aes(xmin = X1 - .3, xmax = X2 + .3, ymin = 0, ymax = cluster_threshold, color = cluster_box.cluster), fill = NA) +\n  geom_segment(data = ggdendro::segment(dtw_data), aes(x = x, y = y, xend = xend, yend = yend)) +\n  coord_flip() +\n  scale_y_continuous(\"Distance\") +\n  scale_x_continuous(\"\", breaks = 1:20, labels = labels_order) +\n  guides(color = FALSE, fill = FALSE) +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(), # remove grids\n    panel.background = element_blank(),\n    axis.text.y = element_text(colour = myPalette[dtw_result$cluster], hjust = 0.5),\n    axis.ticks.y = element_blank()\n  )\n#\np2 &lt;- as.data.frame(matrix(unlist(plan_list),\n  nrow = length(unlist(plan_list[1])),\n  dimnames = list(c(), names(plan_list))\n)) %&gt;%\n  dplyr::mutate(rundatelocal = seq.Date(date_marathon - 175, date_marathon - 1, by = \"day\")) %&gt;%\n  tidyr::gather(key = label, value = value, -rundatelocal) %&gt;%\n  dplyr::mutate(label = as.factor(label)) %&gt;%\n  dplyr::full_join(., dtw_result, by = \"label\") %&gt;%\n  mutate(label = factor(label, levels = rev(as.character(labels_order)))) %&gt;%\n  ggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +\n  geom_line() +\n  geom_area(aes(fill = as.factor(cluster))) +\n  coord_cartesian(ylim = c(0, 50)) +\n  scale_y_continuous(name = \"Total distance per day [km]\", breaks = seq(0, 50, by = 50)) +\n  scale_x_date(name = \"Run Date\", date_breaks = \"4 week\", date_labels = \"%b %d\") +\n  facet_wrap(~label, ncol = 1, strip.position = \"left\") +\n  guides(color = FALSE, fill = FALSE) +\n  theme_bw() +\n  theme(strip.background = element_blank(), strip.text = element_blank())\n#\nplt_list &lt;- list(p2, p1)\nplt_layout &lt;- rbind(\n  c(NA, 2),\n  c(1, 2),\n  c(NA, 2)\n)\n#\ngrid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))\n\n\n\n\n\n\n\n\nThanks to some solutions on Stack Overflow, I think the plot looks good graphically (I’m still working on the label overlap). The results aren’t bad, but some random plans were grouped with the structured plans. Of course, randomness can sometimes produce patterns by chance. It’s also interesting that a higher number of clusters might be needed to achieve a cleaner separation.\n\n\nCentroids\nWe can also examine the centroids to see which plans are most representative of each cluster. This isn’t very useful with only two clusters, but it can be a key tool for distinguishing between many different training plans.\n\ndtw_model_centroids &lt;- data.frame(dtw_model@centroids, rundatelocal = seq.Date(date_marathon - 175, date_marathon - 1, by = \"day\")) %&gt;%\n  tidyr::gather(label, totaldistancekm, starts_with(\"athlete\")) %&gt;%\n  dplyr::left_join(., dtw_result, by = \"label\") %&gt;%\n  dplyr::mutate(label = factor(label, levels = rev(labels_order)))\n#\ndtw_model_centroids %&gt;%\n  ggplot(aes(rundatelocal, totaldistancekm, color = cluster, fill = cluster)) +\n  geom_line() +\n  geom_area() +\n  facet_wrap(~ label + cluster, ncol = 1, strip.position = \"right\", labeller = labeller(.rows = label_both)) +\n  scale_y_continuous(name = \"Total distance per day [km]\") +\n  scale_x_date(name = \"Run Date\", date_breaks = \"4 week\", date_labels = \"%b %d\") +\n  guides(color = FALSE, fill = FALSE) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe main problem with raw data is noise. When trying to extract recurring patterns, random noise can sometimes create meaningless shapes that distort the cluster structure. Since we’re interested in classifying recurring patterns, removing this noise is a good idea. Signal processing offers many techniques for this, such as seasonality decomposition, Hidden Markov Models, and power spectrum analysis."
  },
  {
    "objectID": "posts/list/timeseries-clustering-part2.html#dtw-cluster-with-seasonality-decomposition",
    "href": "posts/list/timeseries-clustering-part2.html#dtw-cluster-with-seasonality-decomposition",
    "title": "Time series clustering with Dynamic Time Warping (Part 2)",
    "section": "DTW cluster with seasonality decomposition",
    "text": "DTW cluster with seasonality decomposition\nWhen it comes to time series analysis in R, certain packages and functions are indispensable. You likely can’t get far without zoo::zoo(), xts::xts(), or tibbletime::as_tbl_time(). However, the base {stats} package contains one of the most useful functions: stl(). This function performs a Seasonal Decomposition of Time Series by Loess, which is a powerful way to separate a time series into its trend, seasonal, and noise components. Here, we’ll use stl() to extract the weekly seasonality from each training plan and then cluster the results with DTW.\nFirst, let’s apply the stl() decomposition to every time series in our list.\n\nextract_seasonality &lt;- function(x, robust) {\n  x_ts &lt;- ts(as.numeric(unlist(x)), frequency = 7)\n  stl_test &lt;- stl(x_ts, s.window = 7, robust)\n  return(stl_test$time.series[, 1])\n}\n#\nplan_seasonality &lt;- plan_list %&gt;%\n  purrr::map(~ extract_seasonality(., robust = TRUE))\n\nNext, we’ll process the model and plot the results.\n\nNclust &lt;- 2\ndtw_model &lt;- dtwclust::tsclust(\n  series = plan_seasonality,\n  type = \"h\",\n  k = Nclust,\n  distance = \"dtw_basic\",\n  control = hierarchical_control(method = \"complete\"),\n  preproc = NULL,\n  # args = tsclust_args(dist = list(window.size = 5L)),\n  trace = TRUE\n)\n\n\nCalculating distance matrix...\nPerforming hierarchical clustering...\nExtracting centroids...\n\n    Elapsed time is 0.048 seconds.\n\n#\ndtw_data &lt;- ggdendro::dendro_data(dtw_model, type = \"rectangle\")\n#\nlabels_order &lt;- dtw_data$labels$label\n#\ndtw_result &lt;- data.frame(\n  label = names(plan_seasonality),\n  cluster = factor(stats::cutree(dtw_model, k = Nclust))\n)\n#\ndtw_data[[\"labels\"]] &lt;- merge(dtw_data[[\"labels\"]], dtw_result, by = \"label\")\ndtw_result &lt;- dplyr::full_join(dtw_result, dtw_data$labels, by = c(\"label\", \"cluster\")) %&gt;%\n  dplyr::arrange(x)\n\n\ncluster_box &lt;- aggregate(x ~ cluster, ggdendro::label(dtw_data), range)\ncluster_box &lt;- data.frame(cluster_box$cluster, cluster_box$x)\ncluster_threshold &lt;- mean(dtw_model$height[length(dtw_model$height) - ((Nclust - 2):(Nclust - 1))])\n#\nnumColors &lt;- length(levels(dtw_result$cluster)) # How many colors you need\ngetColors &lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)\nmyPalette &lt;- getColors(numColors)\nnames(myPalette) &lt;- levels(dtw_result$cluster) # Give every color an appropriate name\n\np1 &lt;- ggplot() +\n  geom_rect(data = cluster_box, aes(xmin = X1 - .3, xmax = X2 + .3, ymin = 0, ymax = cluster_threshold, color = cluster_box.cluster), fill = NA) +\n  geom_segment(data = ggdendro::segment(dtw_data), aes(x = x, y = y, xend = xend, yend = yend)) +\n  coord_flip() +\n  scale_y_continuous(\"Distance\") +\n  scale_x_continuous(\"\", breaks = 1:20, labels = labels_order) +\n  guides(color = FALSE, fill = FALSE) +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(), # remove grids\n    panel.background = element_blank(),\n    axis.text.y = element_text(colour = myPalette[dtw_result$cluster], hjust = 0.5),\n    axis.ticks.y = element_blank()\n  )\n#\np2 &lt;- as.data.frame(matrix(unlist(plan_seasonality),\n  nrow = length(unlist(plan_seasonality[1])),\n  dimnames = list(c(), names(plan_seasonality))\n)) %&gt;%\n  dplyr::mutate(rundatelocal = seq.Date(date_marathon - 175, date_marathon - 1, by = \"day\")) %&gt;%\n  tidyr::gather(key = label, value = value, -rundatelocal) %&gt;%\n  dplyr::mutate(label = as.factor(label)) %&gt;%\n  dplyr::full_join(., dtw_result, by = \"label\") %&gt;%\n  mutate(label = factor(label, levels = rev(as.character(labels_order)))) %&gt;%\n  ggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +\n  geom_line() +\n  geom_area(aes(fill = as.factor(cluster))) +\n  coord_cartesian(ylim = c(-25, 25)) +\n  scale_y_continuous(name = \"Seasonal distance per day [km]\", breaks = seq(-25, 25, by = 50)) +\n  scale_x_date(name = \"Run Date\", date_breaks = \"4 week\", date_labels = \"%b %d\") +\n  facet_wrap(~label, ncol = 1, strip.position = \"left\") +\n  guides(color = FALSE, fill = FALSE) +\n  theme_bw() +\n  theme(strip.background = element_blank(), strip.text = element_blank())\n#\nplt_list &lt;- list(p2, p1)\nplt_layout &lt;- rbind(\n  c(NA, 2),\n  c(1, 2),\n  c(NA, 2)\n)\n#\ngrid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))\n\n\n\n\n\n\n\n\nWell, I think that’s an epic fail. Let’s explore why. Several reasons could explain why we have one cluster with only 3 time series and another with the remaining 17:\n\nI’m only using two clusters. In reality (and with real randomness), many more patterns are possible. Increasing the number of clusters could make the clustering more effective, especially if combined with an evaluation of the optimal cluster count.\nBy removing the noise from the random plans, I inadvertently made them less random, revealing underlying repetitive patterns. This is exactly what I want to do with real data in my research, but here, it just made a mess.\n\nSo, let’s try another method!"
  },
  {
    "objectID": "posts/list/timeseries-clustering-part2.html#dtw-cluster-with-hidden-markov-model",
    "href": "posts/list/timeseries-clustering-part2.html#dtw-cluster-with-hidden-markov-model",
    "title": "Time series clustering with Dynamic Time Warping (Part 2)",
    "section": "DTW cluster with Hidden Markov Model",
    "text": "DTW cluster with Hidden Markov Model\nI’m not an expert in Hidden Markov Models (HMMs), and after looking at the book Hidden Markov Models for Time Series: An Introduction Using R by Zucchini, MacDonald, and Langrock, I can confirm it’s a complex topic. In a nutshell, HMMs cluster values based on their probability of belonging to a hidden “state.”\nIn our case, let’s assume we have three possible states for each day: “no run,” “medium run,” and “long run.” Using an HMM, we can create new time series based on these states instead of distances. This is a qualitative transformation that requires almost no prior assumptions about the state boundaries.\n\nplan_HMM &lt;- as.data.frame(matrix(unlist(plan_list),\n  nrow = length(unlist(plan_list[1])),\n  dimnames = list(c(), names(plan_list))\n)) %&gt;%\n  dplyr::mutate(rundatelocal = seq.Date(date_marathon - 175, date_marathon - 1, by = \"day\")) %&gt;%\n  tidyr::gather(key = label, value = value, -rundatelocal) %&gt;%\n  dplyr::mutate(label = as.factor(label)) %&gt;%\n  dplyr::mutate(value = as.integer(value))\n#\nmod &lt;- depmixS4::depmix(value ~ label, family = poisson(link = \"log\"), nstates = 3, data = plan_HMM)\n#\nfm &lt;- depmixS4::fit(mod, verbose = FALSE)\n\nconverged at iteration 34 with logLik: -10172.1 \n\n#\nprobs &lt;- depmixS4::posterior(fm)\n#\nplan_HMM &lt;- cbind(plan_HMM, probs) %&gt;%\n  dplyr::select(rundatelocal, label, state) %&gt;%\n  tidyr::spread(label, state) %&gt;%\n  dplyr::select(-rundatelocal) %&gt;%\n  purrr::map(~ (.))\n\n\nNclust &lt;- 2\ndtw_model &lt;- dtwclust::tsclust(\n  series = plan_HMM,\n  type = \"h\",\n  k = Nclust,\n  distance = \"dtw_basic\",\n  control = hierarchical_control(method = \"complete\"),\n  preproc = NULL,\n  # args = tsclust_args(dist = list(window.size = 5L)),\n  trace = TRUE\n)\n\n\nCalculating distance matrix...\nPerforming hierarchical clustering...\nExtracting centroids...\n\n    Elapsed time is 0.023 seconds.\n\n#\ndtw_data &lt;- ggdendro::dendro_data(dtw_model, type = \"rectangle\")\n#\nlabels_order &lt;- dtw_data$labels$label\n#\ndtw_result &lt;- data.frame(\n  label = names(plan_HMM),\n  cluster = factor(stats::cutree(dtw_model, k = Nclust))\n)\n#\ndtw_data[[\"labels\"]] &lt;- merge(dtw_data[[\"labels\"]], dtw_result, by = \"label\")\ndtw_result &lt;- dplyr::full_join(dtw_result, dtw_data$labels, by = c(\"label\", \"cluster\")) %&gt;%\n  dplyr::arrange(x)\n\n\ncluster_box &lt;- aggregate(x ~ cluster, ggdendro::label(dtw_data), range)\ncluster_box &lt;- data.frame(cluster_box$cluster, cluster_box$x)\ncluster_threshold &lt;- mean(dtw_model$height[length(dtw_model$height) - ((Nclust - 2):(Nclust - 1))])\n#\nnumColors &lt;- length(levels(dtw_result$cluster)) # How many colors you need\ngetColors &lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)\nmyPalette &lt;- getColors(numColors)\nnames(myPalette) &lt;- levels(dtw_result$cluster) # Give every color an appropriate name\n\np1 &lt;- ggplot() +\n  geom_rect(data = cluster_box, aes(xmin = X1 - .3, xmax = X2 + .3, ymin = 0, ymax = cluster_threshold, color = cluster_box.cluster), fill = NA) +\n  geom_segment(data = ggdendro::segment(dtw_data), aes(x = x, y = y, xend = xend, yend = yend)) +\n  coord_flip() +\n  scale_y_continuous(\"Distance\") +\n  scale_x_continuous(\"\", breaks = 1:20, labels = labels_order) +\n  guides(color = FALSE, fill = FALSE) +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(), # remove grids\n    panel.background = element_blank(),\n    axis.text.y = element_text(colour = myPalette[dtw_result$cluster], hjust = 0.5),\n    axis.ticks.y = element_blank()\n  )\n#\np2 &lt;- as.data.frame(matrix(unlist(plan_HMM),\n  nrow = length(unlist(plan_HMM[1])),\n  dimnames = list(c(), names(plan_HMM))\n)) %&gt;%\n  dplyr::mutate(rundatelocal = seq.Date(date_marathon - 175, date_marathon - 1, by = \"day\")) %&gt;%\n  tidyr::gather(key = label, value = value, -rundatelocal) %&gt;%\n  dplyr::mutate(label = as.factor(label)) %&gt;%\n  dplyr::full_join(., dtw_result, by = \"label\") %&gt;%\n  mutate(label = factor(label, levels = rev(as.character(labels_order)))) %&gt;%\n  ggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +\n  geom_line() +\n  geom_area(aes(fill = as.factor(cluster))) +\n  coord_cartesian(ylim = c(0, 4)) +\n  scale_y_continuous(name = \"States per day [km]\", breaks = seq(0, 4, by = 4)) +\n  scale_x_date(name = \"Run Date\", date_breaks = \"4 week\", date_labels = \"%b %d\") +\n  facet_wrap(~label, ncol = 1, strip.position = \"left\") +\n  guides(color = FALSE, fill = FALSE) +\n  theme_bw() +\n  theme(strip.background = element_blank(), strip.text = element_blank())\n#\nplt_list &lt;- list(p2, p1)\nplt_layout &lt;- rbind(\n  c(NA, 2),\n  c(1, 2),\n  c(NA, 2)\n)\n#\ngrid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))\n\n\n\n\n\n\n\n\nGood news this time: the clusters are almost equally distributed. Bad news: the random and structured plans are mixed together. However, the HMM creates surprisingly clean patterns that could easily be clustered with a higher number of clusters. The main drawback is the low distance between each time series, which could complicate the clustering."
  },
  {
    "objectID": "posts/list/timeseries-clustering-part2.html#dtw-cluster-by-power-spectral-density",
    "href": "posts/list/timeseries-clustering-part2.html#dtw-cluster-by-power-spectral-density",
    "title": "Time series clustering with Dynamic Time Warping (Part 2)",
    "section": "DTW cluster by power spectral density",
    "text": "DTW cluster by power spectral density\nLast but not least, perhaps the best approach for evaluating seasonality in training plans is power spectrum analysis. By identifying the underlying frequencies in each time series, we can cluster them according to their dominant patterns. The excellent {WaveletComp} package is perfect for this, as it analyses the frequency structure of time series using the Morlet wavelet.\n\nNclust &lt;- 2\ndtw_model &lt;- dtwclust::tsclust(\n  series = plan_poweravge,\n  type = \"h\",\n  k = Nclust,\n  distance = \"dtw_basic\",\n  control = hierarchical_control(method = \"complete\"),\n  preproc = NULL,\n  # args = tsclust_args(dist = list(window.size = 5L)),\n  trace = TRUE\n)\n\n\nCalculating distance matrix...\nPerforming hierarchical clustering...\nExtracting centroids...\n\n    Elapsed time is 0.011 seconds.\n\n#\n\ndtw_data &lt;- ggdendro::dendro_data(dtw_model, type = \"rectangle\")\n#\nlabels_order &lt;- dtw_data$labels$label\n#\ndtw_result &lt;- data.frame(\n  label = names(plan_poweravge),\n  cluster = factor(stats::cutree(dtw_model, k = Nclust))\n)\n#\ndtw_data[[\"labels\"]] &lt;- merge(dtw_data[[\"labels\"]], dtw_result, by = \"label\")\ndtw_result &lt;- dplyr::full_join(dtw_result, dtw_data$labels, by = c(\"label\", \"cluster\")) %&gt;%\n  dplyr::arrange(x)\n\n\ncluster_box &lt;- aggregate(x ~ cluster, ggdendro::label(dtw_data), range)\ncluster_box &lt;- data.frame(cluster_box$cluster, cluster_box$x)\ncluster_threshold &lt;- mean(dtw_model$height[length(dtw_model$height) - ((Nclust - 2):(Nclust - 1))])\n#\nnumColors &lt;- length(levels(dtw_result$cluster)) # How many colors you need\ngetColors &lt;- scales::hue_pal() # Create a function that takes a number and returns a qualitative palette of that length (from the scales package)\nmyPalette &lt;- getColors(numColors)\nnames(myPalette) &lt;- levels(dtw_result$cluster) # Give every color an appropriate name\n\np1 &lt;- ggplot() +\n  geom_rect(data = cluster_box, aes(xmin = X1 - .3, xmax = X2 + .3, ymin = 0, ymax = cluster_threshold, color = cluster_box.cluster), fill = NA) +\n  geom_segment(data = ggdendro::segment(dtw_data), aes(x = x, y = y, xend = xend, yend = yend)) +\n  coord_flip() +\n  scale_y_continuous(\"Distance\") +\n  scale_x_continuous(\"\", breaks = 1:20, labels = labels_order) +\n  guides(color = FALSE, fill = FALSE) +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(), # remove grids\n    panel.background = element_blank(),\n    axis.text.y = element_text(colour = myPalette[dtw_result$cluster], hjust = 0.5),\n    axis.ticks.y = element_blank()\n  )\n#\np2 &lt;- as.data.frame(matrix(unlist(plan_poweravge),\n  nrow = length(unlist(plan_poweravge[1])),\n  dimnames = list(c(), names(plan_poweravge))\n)) %&gt;%\n  dplyr::mutate(rundatelocal = 1:n()) %&gt;%\n  tidyr::gather(key = label, value = value, -rundatelocal) %&gt;%\n  dplyr::mutate(label = as.factor(label)) %&gt;%\n  dplyr::full_join(., dtw_result, by = \"label\") %&gt;%\n  mutate(label = factor(label, levels = rev(as.character(labels_order)))) %&gt;%\n  ggplot(aes(x = rundatelocal, y = value, colour = as.factor(cluster))) +\n  geom_line() +\n  geom_area(aes(fill = as.factor(cluster))) +\n  coord_cartesian(ylim = c(0, 1)) +\n  scale_y_continuous(name = \"Average power density\", breaks = seq(0, 1, by = 1)) +\n  scale_x_continuous(name = \"Period (days)\") +\n  facet_wrap(~label, ncol = 1, strip.position = \"left\") +\n  guides(color = FALSE, fill = FALSE) +\n  theme_bw() +\n  theme(strip.background = element_blank(), strip.text = element_blank())\n#\nplt_list &lt;- list(p2, p1)\nplt_layout &lt;- rbind(\n  c(NA, 2),\n  c(1, 2),\n  c(NA, 2)\n)\n#\ngrid.arrange(grobs = plt_list, layout_matrix = plt_layout, heights = c(0.04, 1, 0.05))\n\n\n\n\n\n\n\n\nThis frequency decomposition looks amazing! However, be careful: the power frequencies are averaged. As stated in the package’s guided tour, “[the] average power plot cannot distinguish between consecutive periods and overlapping periods.” This is a limitation, but using average power is definitely a great first step toward a robust classification of training plan patterns."
  },
  {
    "objectID": "posts/list/organising-research-conferences.html",
    "href": "posts/list/organising-research-conferences.html",
    "title": "Organising Academic Conferences with Open Source Tools: Feedback and Reflections",
    "section": "",
    "text": "Last year and again this year, I had the chance to join the organising committees of two separate academic events: the Conference of the International Society for Research on Emotion (#ISRE2024 https://www.isre2024.org/) and the Conference of the Consortium of European Research on Emotion (#CERE2025 https://www.cere2025.com/). For both, I was responsible for the websites and communications. As we were committed to using and encouraging open science practices, the committee and I decided to rely on open-source tools wherever possible. This led me to build the websites using Quarto. For CERE 2025, we also used the platform https://www.sciencesconf.org/ to manage abstract submissions and the review process.\nThis post gives a short overview of the Quarto-based websites, their particular features, the experience with sciencesconf.org, and some broader reflections on what worked and what didn’t during the organisation."
  },
  {
    "objectID": "posts/list/organising-research-conferences.html#quarto-websites-for-conference-management",
    "href": "posts/list/organising-research-conferences.html#quarto-websites-for-conference-management",
    "title": "Organising Academic Conferences with Open Source Tools: Feedback and Reflections",
    "section": "1. Quarto Websites for Conference Management",
    "text": "1. Quarto Websites for Conference Management\nA conference website doesn’t differ that much from a personal one. Quarto turned out to be a solid fit: free, fast to update, and flexible. You can generate tables, automate pages, and embed code wherever needed. I won’t walk through the full site structure (that’s all on GitHub here for ISRE 2024 and here for CERE 2025), but I’ll highlight two features I found particularly useful.\nFirst, the automatic generation of pages for parallel sessions. Most conferences run several sessions at once, often grouped thematically or by format, such as symposia or individual talks. These need to be listed clearly, but the problem is that schedules change constantly until the last minute. Manually creating or editing dozens of pages is a waste of time. So, I built a workflow using a Quarto template with a {purrr} loop that reads a spreadsheet of the programme and write a .qmd page before Quarto build it as .html in the website. This allowed the site to generate or update pages dynamically, making the process far more efficient and much less error-prone.\n\n\n\n~/cere2025/internal/computation_pages.R\n\n# List all the parallel sessions\nsessions &lt;- parallel_sessions |&gt;\n  distinct(session, track)\n\n# Read the quarto template\ntemplate &lt;- readr::read_file(\"internal/parallel_session_template.qmd\")\n\n# Create a .qmd file per session\npage_creation &lt;- function(session, track) {\n  file_conn &lt;-\n    glue::glue(\"program/{paste(snakecase::to_any_case(session), track, sep = '_')}.qmd\") |&gt;\n    file(\"w\")\n\n  writeLines(\n    glue::glue(\n      template,\n      .open = \"{{\", .close = \"}}\"\n    ), # glue double fenced because of code chunk capsule\n    con = file_conn\n  )\n\n  close(file_conn)\n}\n\npurrr::pwalk(sessions, page_creation)\n\n\n\n\n\n\n\n\nCaution\n\n\n\nExclude the folder /internal from _quarto.yml but call the script using the option pre-render:\n\n\n\n~/cere2025/_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs\n  render:\n    - \"*.qmd\"\n    - \"!internal/\"\n  pre-render: internal/computation_pages.R\n\n\n\n\nSecond, the abstract book. Like the sessions, the book needed to group all the abstracts per session. Again, it made no sense to hardcode each one. The same kind of loop was used here, this time with knit_child() to plug each abstract into a larger template. The result was a clean, automatically generated PDF covering all contributions, updated in seconds if anything changed.\n\n\n\n~/cere2025/internal/pdf_program.qmd\n\nsessions &lt;- parallel_sessions |&gt;\n  distinct(session, track)\n\nres &lt;- map2(\n  sessions$session, sessions$track,\n  ~ knit_child(\n    here(\"internal/pdf_template.qmd\"),\n    envir = environment(),\n    quiet = TRUE\n  )\n)\ncat(unlist(res), sep = \"\\n\")"
  },
  {
    "objectID": "posts/list/organising-research-conferences.html#using-sciencesconf.org-for-abstract-management",
    "href": "posts/list/organising-research-conferences.html#using-sciencesconf.org-for-abstract-management",
    "title": "Organising Academic Conferences with Open Source Tools: Feedback and Reflections",
    "section": "2. Using sciencesconf.org for Abstract Management",
    "text": "2. Using sciencesconf.org for Abstract Management\nFor ISRE 2024, we used Microsoft’s CMT system, similar to EasyChair. For CERE 2025, we switched to sciencesconf.org. While both are free, sciencesconf.org is developed by a French research agency and open to international use. If you’re concerned about private providers like Microsoft or data server locations, it’s a decent alternative.\nSciencesconf.org can act as your main website, your payment system, or your submission and review platform. We used it just for the latter, as we already had a better website and a payment solution provided by the host university.\nIts main advantage is simple: it costs nothing. It covers all basic needs for submission and review, including abstract uploads, reviewer management, and author communication. But the interface is bare and, at times, clunky. It also struggles with more complex submission formats. For instance, our conference allowed symposia submissions, which can contain up to five abstracts. There was no clean way to gather and display that structure properly.\nSome features also require extra care. By default, it doesn’t collect co-author emails, only names and affiliations. You have to tick a specific box if you want co-authors to receive decision letters. The email system is another small hurdle: sciencesconf.org generates a new conference-specific email address, so if you already have an official one, you’ll need to redirect replies by setting it up in sciencesconf.org to avoid missing messages.\nI also found the reviewer interface quite awkward. It functions, but the design is poor and doesn’t look especially professional. Still, given its zero cost and openness, I would use it again—albeit with reservations."
  },
  {
    "objectID": "posts/list/organising-research-conferences.html#general-observations-and-lessons",
    "href": "posts/list/organising-research-conferences.html#general-observations-and-lessons",
    "title": "Organising Academic Conferences with Open Source Tools: Feedback and Reflections",
    "section": "3. General Observations and Lessons",
    "text": "3. General Observations and Lessons\n\nOne area that proved especially tricky was building the actual conference programme. This should be a collaborative effort, but not everyone uses a shared, editable spreadsheet. That creates confusion. Using something like Google Sheets, with full access for the relevant team members, should be standard practice.\nTo organise the programme, talks need to be grouped by topic, followed by assigning each group a session title. I experimented with GenAI tools like ChatGPT and Gemini to help with this, but since both rely on cosine similarity measures, the results weren’t convincing. The groupings felt off, and the suggested titles lacked quality.\nDespite spending time building a web page for each parallel session, most attendees ended up referring to the overall PDF Gantt-style or long table schedule. Printing and displaying it on the doors of the venue was the best way to communicate it.\nTo finish, a quick note on social media. We ran a Twitter/X account with over 2,500 followers. Engagement was minimal. It may be time to stop treating X as a serious platform for promoting conferences. The audience is no longer there and, instead, LinkedIn might be the most suitable place to communicate.\n\nIf you’re organising a conference, open-source tools can save time and money, but they’re not perfect. Knowing when to automate and when to stick to something simpler is key."
  },
  {
    "objectID": "posts/list/generating-multiple-pdf.html",
    "href": "posts/list/generating-multiple-pdf.html",
    "title": "Generating Multiple PDF at Once: A use case of Rmarkdown render in a {purrr} loop",
    "section": "",
    "text": "Note\n\n\n\nThis blog post was first published on my previous website in 2019 and has since been lightly revised.\nFollowing on from my earlier post about generating PDFs with R Markdown and a .docx template, a few people asked for more details on how I use R Markdown in day-to-day work. I’ve got a few examples worth sharing, and one I use regularly is batch-generating PDF reports. It’s simple, efficient, and solves a real problem.\nHere’s the situation. For some of the modules I teach, I ask students to write short research papers and submit them as PDFs. That’s fine, but annotating PDFs to give feedback is awkward. My workaround is to create a feedback PDF for each student, with comments on their work section by section.\nNow, I could manually write and export one PDF per student, but that quickly becomes tedious. Instead, I write my comments in a spreadsheet and then use that to automatically generate all the PDFs at once.\nThe approach is borrowed from Alison Hill’s presentation “Made with YAML, strings, and glue. An R Markdown valentine for you, which is excellent. I’m just applying it to a different case, so if you want a deeper guide, I’d recommend reading her slides. Still, seeing it in a teaching context might be useful.\nLet’s get to the code."
  },
  {
    "objectID": "posts/list/generating-multiple-pdf.html#step-1-prepare-the-data",
    "href": "posts/list/generating-multiple-pdf.html#step-1-prepare-the-data",
    "title": "Generating Multiple PDF at Once: A use case of Rmarkdown render in a {purrr} loop",
    "section": "Step 1: Prepare the data",
    "text": "Step 1: Prepare the data\nHere are the packages that you need:\n\nlibrary(tidyverse)\nlibrary(knitr)\n\nTo keep things light, I’m using a table of TV characters and quotes instead of real student data. Here’s the data frame:\n\nms_1 &lt;- \"Sometimes I’ll start a sentence and I don’t even know where it’s going. I just hope I find it along the way.\"\nms_2 &lt;- \"I’m not superstitious, but I am a little stitious.\"\nms_3 &lt;- \"Would I rather be feared or loved? Easy. Both. I want people to be afraid of how much they love me.\"\nlk_1 &lt;- \"We have to remember what’s important in life: friends, waffles, and work. Or waffles, friends, work. But work has to come third.\"\nlk_2 &lt;- \"What I hear when I’m being yelled at is people caring really loudly at me.\"\nlk_3 &lt;- \"There’s nothing we can’t do if we work hard, never sleep, and shirk from all other responsibilities in our lives.\"\njp_1 &lt;- \"Fine, but in protest, I’m walking over there extremely slowly!\"\njp_2 &lt;- \"I wasn't hurt that badly. The doctor said all my bleeding was internal. That's where the blood's supposed to be.\"\njp_3 &lt;- \"I appealed to their sense of teamwork and camaraderie with a rousing speech that would put Shakespeare to shame.\"\n\ntribble(\n  ~fullname, ~quote_1, ~quote_2, ~quote_3,\n  \"Michael Scott\", ms_1, ms_2, ms_3,\n  \"Leslie Knope\", lk_1, lk_2, lk_3,\n  \"Jake Peralta\", jp_1, jp_2, jp_3\n) |&gt;\n  kable()\n\nSave this as data_batch.csv. It’s the input for the batch process."
  },
  {
    "objectID": "posts/list/generating-multiple-pdf.html#step-2-build-the-rmd-template",
    "href": "posts/list/generating-multiple-pdf.html#step-2-build-the-rmd-template",
    "title": "Generating Multiple PDF at Once: A use case of Rmarkdown render in a {purrr} loop",
    "section": "Step 2: Build the Rmd template",
    "text": "Step 2: Build the Rmd template\nCreate a file called index.Rmd. This is the template that will generate one PDF per person. The YAML at the top sets things up:\n\n---\ntitle: \"Batch PDF processing - `r params$fullname` quotes\"\noutput: pdf_document\nparams:\n  fullname: \"TV Show Character\"\n---\n\nThe key bit is params:. It tells RMarkdown to expect a value (in this case, a name) to insert wherever you call params$fullname. It also allows you to filter the input data, so each report is personalised.\nWithin the body of the template, read the CSV and filter it down to just the row that matches params$fullname. Then pull in the quotes.\nYou can do this using inline R code like this:\n\n`r filtered_data$quote_1`\n\nOr, if you prefer, use the package {epoxy} by Garrick Aden-Buie. It’s great for longer texts with lots of variables. In this case, it’s probably overkill, but still a good tool to know."
  },
  {
    "objectID": "posts/list/generating-multiple-pdf.html#step-3-the-batch-script",
    "href": "posts/list/generating-multiple-pdf.html#step-3-the-batch-script",
    "title": "Generating Multiple PDF at Once: A use case of Rmarkdown render in a {purrr} loop",
    "section": "Step 3: The batch script",
    "text": "Step 3: The batch script\nNow, the script that does the heavy lifting. Save this as _render_batch.R:\n\ndata_batch &lt;- read_csv(\"data_batch.csv\")\n\nwalk(\n  .x = data_batch$fullname,\n  ~ render(\n    input = \"index.Rmd\",\n    output_file = glue::glue(\"PDF output - {.x} quotes.pdf\"),\n    params = list(fullname = {\n      .x\n    })\n  )\n)\n\nwalk() takes a list and applies a function to each element. In this case, it runs render() for each character name. The params argument tells it which row to pull from the CSV, and glue::glue() ensures each output file gets a unique name.\nThat’s it. No overwriting, no manual labour. One spreadsheet in, a stack of PDFs out."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nGenerate PDF documents using R Markdown with a .docx letter template\n\n\n\n\n\n\nGenerating Multiple PDF at Once: A use case of Rmarkdown render in a {purrr} loop\n\n\n\n\n\n\nHow to Organise a Wedding Using R: Google Search API, Google Drive, Web Scraping, and Automated Emails\n\n\n\n\n\n\nOrganising Academic Conferences with Open Source Tools: Feedback and Reflections\n\n\n\n\n\n\nTime series Clustering with Dynamic Time Warping\n\n\n\n\n\n\nTime series clustering with Dynamic Time Warping (Part 2)\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/index.html",
    "href": "courses/index.html",
    "title": "Courses",
    "section": "",
    "text": "BAA1028 - Workflow & Data Management\n\n\n\n\n\n\n\n\n\n\n\n\nBAA1030 - Data Analytics and Story Telling\n\n\n\n\n\n\n\n\n\n\n\n\nMT612 - Advanced Quantitative Research Methods\n\n\n\n\n\n\n\n\n\n\n\n\nSTA1005 - Quantitative Research Methods\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "courses/details/STA1005.html",
    "href": "courses/details/STA1005.html",
    "title": "STA1005 - Quantitative Research Methods",
    "section": "",
    "text": "This module has been designed to help you improve your understanding of the key quantitative tools and statistical analyses for social science and business research. To start you will explore the key concepts and issues in measurement, focusing in particular on numeric variables and multi-item scales. The main focus of the course is a study of the main data analysis techniques based on linear regressions and multivariate linear regressions. This will instruct you in statistical test selection using SPSS, JAMOVI (open-source alternative to SPSS) and R (short introduction). Finally the module will present how to interpret and to report the results of these tests.\n\n\n\n\n\nLink to the lecture slides:\n\n1: Statistics, Research Papers and Me\n2: Understanding Models and Equations\n3: Collect, Clean, and Transform Data\n4: Understanding the General Linear Model\n5: Categories in the General Linear Model\n6: Assumptions of the General Linear Model\n7: Introduction to R for Hypothesis Testing\n8: Introduction to Quarto for Research"
  },
  {
    "objectID": "courses/details/BAA1028.html",
    "href": "courses/details/BAA1028.html",
    "title": "BAA1028 - Workflow & Data Management",
    "section": "",
    "text": "By developing skills and gaining hands-on experience in designing and implementing cloud-based databases and workflows, students will acquire proficiency in some of the most widely used technologies for managing data analytics within organisations.\nTo support this, students will be introduced to modern tools for data management and web development (e.g., HTML, CSS, Markdown), cloud computing platforms (e.g., the AWS ecosystem), and version control systems (e.g., Git and GitHub). They will also gain a solid understanding of data security, including strategies to safeguard API keys and other sensitive information from breaches.\n\n\n\n\n\nLink to the lecture slides:\n\n1. Introduction to ePortfolios\n2. HTML and CSS\n3. Publishing with GitHub Pages\n4. Git, GitHub, and Version Control\n5. The Final Workflow of Git, GitHub and Quarto with VS Code\n6. Advanced Quarto for Website Creation\n7. Customizing Quarto Websites\n8. HTML Theming in a Quarto Website\n9. Displaying Projects in a Quarto Website\n10. Quarto Dashboards\n11. Python Apps without Server\n12. Python in your Project Pages"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "PhD in Social and Experimental Psychology from the University Grenoble-Alpes, France. My research centres on psycho-physiological responses in real-world settings. Working with Anna Tcherkassof in Grenoble, I helped develop the DynEmo database by recording and analysing dynamic, spontaneous facial expressions of emotion.\nMy thesis focused on evaluating Emotional User eXperience of new technologies for the innovation consultancy firm Ixiade (Grenoble, France). Alongside that, I collaborated with Queen’s University Belfast and Sensum Ltd. (Belfast, UK), using physiological sensors and automated facial expression recognition to explore emotional responses. I also worked at the Insight Centre for Data Analytics at University College Dublin, where I processed physiological data from marathon runners to better understand stress and effort in endurance contexts.\nCurrently Assistant Professor of Business Research Methods at Dublin City University, I specialise in multivariate time series analysis and trend extraction, applying these techniques to both supervised and unsupervised machine learning classification tasks."
  },
  {
    "objectID": "courses/details/BAA1030.html",
    "href": "courses/details/BAA1030.html",
    "title": "BAA1030 - Data Analytics and Story Telling",
    "section": "",
    "text": "Data and visual analytics is an emerging field concerned with analysing, modelling, and visualising complex high dimensional data. This course will introduce students to the field by covering state of the art modelling, analysis, and visualisation techniques. It will emphasise practical challenges involving complex real world data and include several case studies and hands on work with programming languages (Python and Markdown) and visualisation software (Tableau and PowerBI).\n\n\n\n\n\nLink to the lecture slides:\n\n1: Introduction\n2: Data Collection, Storage and Access\n3: Principles of Data Visualisation\n4: Introduction to Power BI\n5: Introduction to Tableau\n6: Advanced Features in Tableau\n7: Introduction to Python\n8: Data Wrangling with Polars\n9: Visualisation in python with plotnine\n10: Introduction to Quarto\n11: Publish your Report with GitHub"
  },
  {
    "objectID": "courses/details/mt612.html",
    "href": "courses/details/mt612.html",
    "title": "MT612 - Advanced Quantitative Research Methods",
    "section": "",
    "text": "This module is designed for students already mastering linear regression models, to give them a deeper knowledge of more advanced models: logistic regressions, multilevel / hierarchical models, and paths analyses (mediation, factor analysis, structural equation model). Each of these advanced models will be taught with JAMOVI (open-source software for advanced statistics and with R (open-source programming language for advanced statistics). The module will focus on how to test hypotheses using these models, how to interpret their results and how to communicate them in academic publications.\n\n\n\n\n\nLink to the lecture slides:\n\n1: The General Linear Model\n2: The Generalised Linear Model\n3: Linear Mixed Models\n4: Generalized Additive (Mixed) Models\n5: Path Analysis for Mediation Hypotheses\n6: From Path Analysis to SEM"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Damien Dupré, PhD",
    "section": "",
    "text": "Welcome to my Data Science blog! You will find links to my research, as well as the support of the courses that I’m teaching.\nLearn more about me →"
  },
  {
    "objectID": "posts/list/generate-pdf-documents.html",
    "href": "posts/list/generate-pdf-documents.html",
    "title": "Generate PDF documents using R Markdown with a .docx letter template",
    "section": "",
    "text": "Note\n\n\n\nThis blog post was first published on my previous website in 2019 and has since been lightly revised.\nI keep discovering new ways to use R Markdown, and this one’s worth sharing: generating PDF documents using RMarkdown from a .docx letter template.\nPart of my job involves producing letters that follow the same structure but differ in specific details. R Markdown parameters are perfect for this kind of repetition. If you’ve never used them before, Xie, Allaire, and Grolemund’s R Markdown: The Definitive Guide is a good place to start. The challenge, though, is making the output look right, especially if you want a PDF that matches your own letterhead or house style.\nUsing output: pdf_document gives you a plain page. There are template packages around, like those listed in the R Markdown Gallery, but they rarely let you use your own branded background or header-footer setup.\nHere’s my workaround. Start with a Word document and use the output: word_document format. Include your own template using reference_docx: \"your_template.docx\". Then convert that to PDF. Just note: only the header and footer elements from the Word file will carry over into the final PDF. Text and images in the main body won’t survive this conversion.\nYou can manually open the Word file and save it as a PDF, but it’s cleaner to build the conversion into the knitting process. Here’s how that looks in the YAML header:\nThe output_file argument names the Word file you’ll generate using your custom template. To run a second step after knitting, you use a semicolon. It’s not the most elegant syntax, but it works here to add more R code inside the YAML.\nFor the PDF conversion, I used the {doconv} package by David Gohel. It supports two backends: LibreOffice and Python’s docx2pdf. Only the latter preserves headers and footers properly in the final PDF, so you’ll need Python 3 installed. Run doconv::docx2pdf_install() to get everything set up.\nOnce it’s working, the knitted PDF will match your Word template, using the same file name.\nThat said, I ran into issues with Mac M1 machines. On those, the docx2pdf() function couldn’t locate the library. You can work around this by finding the actual path to the tool with which docx2pdf in Terminal, then specifying that full path:"
  },
  {
    "objectID": "posts/list/generate-pdf-documents.html#edit",
    "href": "posts/list/generate-pdf-documents.html#edit",
    "title": "Generate PDF documents using R Markdown with a .docx letter template",
    "section": "Edit",
    "text": "Edit\nYou can avoid the system() workaround on Mac M1 by making sure the correct Python environment is used.\nThe issue seems to come from mismatched Python setups. The {doconv} package uses the {locatexec} package to find the right Python executable with python_exec(), but this might not be the one that holds your installed docx2pdf library.\nTo fix this, copy the path returned by which docx2pdf and paste that file into the folder returned by dirname(locatexec::python_exec()). That way, everything stays reproducible and you’re not relying on hardcoded paths that only work on your machine."
  },
  {
    "objectID": "posts/list/how-to-organise.html",
    "href": "posts/list/how-to-organise.html",
    "title": "How to Organise a Wedding Using R: Google Search API, Google Drive, Web Scraping, and Automated Emails",
    "section": "",
    "text": "Note\n\n\n\nThis post first appeared on my {blogdown} site, now archived. After a recent request from someone looking to use the same approach, I decided to bring it back on my current Quarto site, unchanged from the original.\nSee Section 1.4 for my latest notes.\nPlanning a wedding is a challenge. For R users, we have one advantage: automation. One of the trickiest parts is finding a venue. There are plenty, but many will already be booked for your date. Here is how I created a list of potential venues with the Google Search API, stored it on Google Drive, scraped emails, and sent messages, all with R."
  },
  {
    "objectID": "posts/list/how-to-organise.html#creating-a-venue-list-with-the-google-places-api",
    "href": "posts/list/how-to-organise.html#creating-a-venue-list-with-the-google-places-api",
    "title": "How to Organise a Wedding Using R: Google Search API, Google Drive, Web Scraping, and Automated Emails",
    "section": "Creating a venue list with the Google Places API",
    "text": "Creating a venue list with the Google Places API\nSince R offers a package for almost everything, I used {googleway} to pull venue data from Google Places. This API includes several services like Directions, Geolocation and Places. To use it, you must register a card on Google Cloud to get an API key. Used moderately, this is free. I found this Stack Overflow answer helpful when learning googleway.\n\nTargeted cities\nI wanted my wedding in the Auvergne-Rhone-Alpes region of France. A single search term like “Auvergne-Rhone-Alpes” might not catch all options, so I built a loop that searches by city. My list of cities comes from their department codes (e.g. the department codes 01, 07, 26, 38, 69, 73 and 74 correspond to Ain, Ardèche, Drôme, Isère, Rhône, Savoie and Haute-Savoie in France).\n\ndept_target &lt;- c(01, 07, 26, 38, 69, 73, 74)\n#\nlist_city &lt;- read.csv(\n  file = url(\"https://sql.sh/ressources/sql-villes-france/villes_france.csv\"),\n  header = FALSE\n) %&gt;%\n  dplyr::select(dept = V2, city = V5, pop2010 = V15) %&gt;%\n  dplyr::mutate(city = as.character(city)) %&gt;%\n  dplyr::filter(dept %in% dept_target) %&gt;% # filter by target departments\n  dplyr::filter(pop2010 &gt; 5000) %&gt;% # filter by city population size\n  magrittr::use_series(city)\n\n\n\nQuerying Google Places\nOnce the cities are ready, I run a loop querying Google Places for each one. If a next page token is found, the script fetches results from subsequent pages until all results are retrieved.\n\ndf_places_final &lt;- NULL\nfor (city in list_city) {\n  # print(city)\n\n  df_places &lt;- googleway::google_places(\n    search_string = paste(\"mariage\", city, \"france\"),\n    key = google_key$key\n  ) # replace by your Google API key\n\n  if (length(df_places$results) == 0) next\n\n  df_places_results &lt;- df_places$results\n  geometry &lt;- df_places_results$geometry$location\n  df_places_results &lt;- df_places_results[, c(\"name\", \"formatted_address\", \"place_id\", \"types\")]\n  df_places_results &lt;- cbind(df_places_results, geometry)\n\n  while (!is.null(df_places$next_page_token)) {\n    df_places &lt;- googleway::google_places(\n      search_string = paste(\"mariage\", city, \"france\"),\n      page_token = df_places$next_page_token,\n      key = google_key$key\n    )\n\n    df_places_next &lt;- df_places$results\n\n    if (length(df_places_next) &gt; 0) {\n      geometry &lt;- df_places_next$geometry$location\n      df_places_next &lt;- df_places_next[, c(\"name\", \"formatted_address\", \"place_id\", \"types\")]\n      df_places_next &lt;- cbind(df_places_next, geometry)\n      df_places_results &lt;- rbind(df_places_results, df_places_next)\n    }\n    Sys.sleep(2) # time to not overload  the google API\n  }\n  df_places_final &lt;- rbind(df_places_final, df_places_results)\n}\n\nThe raw results include caterers, photographers and shops. I filtered them to keep only venues such as castles, mansions and estates. Duplicates are also removed.\n\ndf_places_filtered &lt;- df_places_final %&gt;%\n  dplyr::filter(grepl(\"castle|chateau|domaine|manoir|ferme\", name, ignore.case = TRUE)) %&gt;%\n  dplyr::distinct(place_id, .keep_all = TRUE)\n\nWith {leaflet}, I visualised the locations on a map.\n\nleaflet() %&gt;%\n  addTiles() %&gt;% # Add default OpenStreetMap map tiles\n  addMarkers(lng = df_places_filtered$lng, lat = df_places_filtered$lat, popup = df_places_filtered$name)\n\n\n\nGetting venue websites\nThe first API call does not return website URLs, but google_place_details() does. Using {purrr}, I applied a small function to fetch them.\n\nget_website &lt;- function(place_id) {\n  # print(place_id)\n  place_id &lt;- as.character(place_id)\n  dat &lt;- googleway::google_place_details(place_id = place_id, key = google_key$key)\n  res &lt;- ifelse(is.null(dat$result$website), \"no_website\", dat$result$website)\n  return(res)\n}\n\nwebsite_list &lt;- df_places_filtered$place_id %&gt;%\n  purrr::map(get_website) %&gt;%\n  unlist()\ndf_places_filtered$website &lt;- website_list\n\nI removed venues without websites and cleaned up the remaining URLs for later use in web scraping.\n\ndf_places_filtered &lt;- df_places_filtered %&gt;%\n  dplyr::filter(website != \"no_website\") %&gt;%\n  dplyr::mutate(website = gsub(\"\\\\,.*\", \"\", website)) %&gt;%\n  dplyr::mutate(website = gsub(\"com/fr\", \"com\", website)) %&gt;%\n  dplyr::mutate(website_contact = paste0(website, \"contact\"))\n\nThe list of venues is now “clean” we can start the web scraping to obtain venues’ emails."
  },
  {
    "objectID": "posts/list/how-to-organise.html#scraping-websites-for-emails",
    "href": "posts/list/how-to-organise.html#scraping-websites-for-emails",
    "title": "How to Organise a Wedding Using R: Google Search API, Google Drive, Web Scraping, and Automated Emails",
    "section": "Scraping websites for emails",
    "text": "Scraping websites for emails\nGoogle does not provide emails, so I scraped the websites using {rvest}. Most venues list emails on their home or contact page. A simple function handles this, with tryCatch() to skip broken URLs.\n\nextract_email &lt;- function(website) {\n  # print(website)\n  url_test &lt;- tryCatch(xml2::read_html(website), error = function(e) print(\"url_error\"))\n  if (url_test == \"url_error\") {\n    return(NA)\n  } else {\n    text_web &lt;- xml2::read_html(website) %&gt;%\n      rvest::html_text()\n    email_text &lt;- unlist(regmatches(text_web, gregexpr(\"([_a-z0-9-]+(\\\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\\\.[a-z0-9-]+)*(\\\\.[a-z]{2,4}))\", text_web)))\n    email_text &lt;- gsub(\"\\n\", \"\", email_text)\n    email_text &lt;- gsub(\" \", \"\", email_text)\n    return(email_text[1])\n  }\n}\n# web scraping home page\nemail_list &lt;- df_places_filtered$website %&gt;%\n  purrr::map(extract_email) %&gt;%\n  unlist()\ndf_places_filtered$email &lt;- email_list\n# web scraping contact page\nemail_list &lt;- df_places_filtered$website_contact %&gt;%\n  purrr::map(extract_email) %&gt;%\n  unlist()\ndf_places_filtered$email_contact &lt;- email_list\n# merge email and email_contact\ndf_places_filtered &lt;- df_places_filtered %&gt;%\n  dplyr::mutate(email = ifelse(is.na(email), email_contact, email)) %&gt;%\n  dplyr::filter(!is.na(email)) %&gt;%\n  dplyr::select(-email_contact, -types)\n\n\ndf_places_filtered %&gt;%\n  dplyr::select(name, website) %&gt;%\n  DT::datatable(options = list(pageLength = 5))\n\nWe now have a list of venues to contact."
  },
  {
    "objectID": "posts/list/how-to-organise.html#google-drive-and-automated-emails",
    "href": "posts/list/how-to-organise.html#google-drive-and-automated-emails",
    "title": "How to Organise a Wedding Using R: Google Search API, Google Drive, Web Scraping, and Automated Emails",
    "section": "Google Drive and automated emails",
    "text": "Google Drive and automated emails\nIt helps to create a separate email account just for wedding planning. Google makes this easy and also offers Google Drive for storing documents. With the {googledrive} package, sharing and updating files with your partner is straightforward (see https://googledrive.tidyverse.org/index.html for some information about {googledrive}).\n\nUploading the list to Google Drive\nFirst, save the data frame locally, then upload.\n\n# first save the list of venues local\nwrite.csv(df_places_filtered, \"list_venues.csv\", row.names = FALSE)\n# upload to google drive\ndrive_upload(media = \"list_venues.csv\", name = \"list_venues\", type = \"spreadsheet\")\n\n\n\nDownloading from Google Drive\nYou can then download and reload the file when needed.\n\n# select file id from google drive\nlist_venues_id &lt;- drive_find() %&gt;%\n  dplyr::filter(name == \"list_venues\") %&gt;%\n  magrittr::use_series(id)\n# download list of venues locally\ndrive_download(as_id(list_venues_id), overwrite = TRUE, type = \"csv\")\n# read local list of venues file\nlist_venues &lt;- read.csv(\"list_venues.csv\", row.names = NULL) %&gt;%\n  dplyr::mutate_if(is.factor, as.character)\n\n\n\nSending emails\nWith the list ready, I sent emails in a simple loop. The script extracts each venue name and email and sends a standard message asking about availability. Make sure to allow less secure apps in Gmail settings.\n\nemail_to_send &lt;- list_venues\n#\n# Email to send\nemail_text &lt;- \"&lt;p&gt;Dear owner/manager of '{name}', &lt;br&gt;&lt;br&gt;We are contacting you because we would like to organise our wedding &lt;b&gt;Sunday 9 of June 2019&lt;/b&gt; and your plac would be amazing for it.&lt;br&gt;&lt;br&gt;That's why we would like to know if your venue '{name}' is available &lt;b&gt;Sunday 9 of June 2019&lt;/b&gt;?&lt;/b&gt;&lt;br&gt;&lt;br&gt;Best regards,&lt;br&gt;&lt;br&gt;YOUR NAMES&lt;/p&gt;\"\n#\nfor (i in 1:nrow(email_to_send)) {\n  df &lt;- email_to_send[i, ]\n  name &lt;- as.character(df$name)\n  ################################\n  send.mail(\n    from = gmail_wedding$email,\n    to = as.character(df$email),\n    subject = \"Availability for a wedding on the 09/06/2019\",\n    body = glue::glue(email_text),\n    smtp = list(\n      host.name = \"smtp.gmail.com\", port = 465,\n      user.name = gmail_wedding$email,\n      passwd = gmail_wedding$passwd, ssl = TRUE\n    ),\n    authenticate = TRUE,\n    send = TRUE,\n    html = TRUE\n  )\n}\n\nAfter sending, I updated the contact date in the data to avoid duplicates.\n\nemail_to_send &lt;- email_to_send %&gt;%\n  dplyr::mutate(date_contact = as.character(as.Date(Sys.Date()))) %&gt;%\n  dplyr::mutate(type_contact = \"automatic email\")\n# Checks in case of different batch of email sending\nid &lt;- match(list_venues$name, email_to_send$name, nomatch = 0L)\nlist_venues$date_contact[id != 0] &lt;- email_to_send$date_contact[id]\nlist_venues$type_contact[id != 0] &lt;- email_to_send$type_contact[id]\n# Write data on local and Upload data from local to google drive\nwrite.csv(list_venues, \"ist_venues.csv\", row.names = FALSE)\ndrive_update(file = \"list_venues\", media = \"list_venues.csv\")\n\nI hope these scripts help you find the perfect venue. Best of luck with your planning."
  },
  {
    "objectID": "posts/list/how-to-organise.html#sec-updates",
    "href": "posts/list/how-to-organise.html#sec-updates",
    "title": "How to Organise a Wedding Using R: Google Search API, Google Drive, Web Scraping, and Automated Emails",
    "section": "Updates and Comments",
    "text": "Updates and Comments\nSince I wrote this code in 2018, things have changed. I would rely less on for loops and use {purrr} map or walk functions instead.\nTo send batch emails with Gmail, I now use {blastula} instead of {mailR}."
  },
  {
    "objectID": "posts/list/timeseries-clustering-part1.html",
    "href": "posts/list/timeseries-clustering-part1.html",
    "title": "Time series Clustering with Dynamic Time Warping",
    "section": "",
    "text": "If you want to cluster time series in R, you’re in luck. There are many available solutions, and the web is packed with helpful tutorials like those from Thomas Girke, Rafael Irizarry and Michael Love, Andrew B. Collier, Peter Laurinec, Dylan Glotzer, and Ana Rita Marques.\nDynamic Time Warping (DTW) is one of the most popular solutions. Its primary strength is that it can group time series by shape, even when their patterns are out of sync or lagged.\nFrom what I’ve seen, {TSclust} by Pablo Montero Manso and José Antonio Vilar and {dtwclust} by Alexis Sarda-Espinosa are the two go-to packages for this task. They’re both simple and powerful, but understanding how they work on real data can be tricky. To demystify the process, I’ll simulate two distinct groups of time series and see if DTW clustering can tell them apart."
  },
  {
    "objectID": "posts/list/timeseries-clustering-part1.html#dtw-cluster",
    "href": "posts/list/timeseries-clustering-part1.html#dtw-cluster",
    "title": "Time series Clustering with Dynamic Time Warping",
    "section": "DTW cluster",
    "text": "DTW cluster\nThe workflow for both {TSclust} and {dtwclust} involves the same general steps:\n\nCompute a dissimilarity matrix for all time series pairs using a distance metric like DTW (as described by Montero & Vilar, 2014).\nApply hierarchical clustering to the dissimilarity matrix.\nGenerate a dendrogram to visualize the cluster results. The technique for plotting the time series next to the dendrogram comes from Ian Hansel’s blog.\n\n\nUsing {TSclust}\n\n# cluster analysis\ndist_ts &lt;- TSclust::diss(SERIES = t(ts_sim_df), METHOD = \"DTWARP\") # note the dataframe must be transposed\nhc &lt;- stats::hclust(dist_ts, method = \"complete\") # meathod can be also \"average\" or diana (for DIvisive ANAlysis Clustering)\n# k for cluster which is 2 in our case (classic vs. wall)\nhclus &lt;- stats::cutree(hc, k = 2) %&gt;% # hclus &lt;- cluster::pam(dist_ts, k = 2)$clustering has a similar result\n  as.data.frame(.) %&gt;%\n  dplyr::rename(., cluster_group = .) %&gt;%\n  tibble::rownames_to_column(\"type_col\")\n\nhcdata &lt;- ggdendro::dendro_data(hc)\nnames_order &lt;- hcdata$labels$label\n# Use the folloing to remove labels from dendogram so not doubling up - but good for checking hcdata$labels$label &lt;- \"\"\n\np1 &lt;- hcdata %&gt;%\n  ggdendro::ggdendrogram(., rotate = TRUE, leaf_labels = FALSE)\n\np2 &lt;- ts_sim_df %&gt;%\n  dplyr::mutate(index = 1:420) %&gt;%\n  tidyr::gather(key = type_col, value = value, -index) %&gt;%\n  dplyr::full_join(., hclus, by = \"type_col\") %&gt;%\n  mutate(type_col = factor(type_col, levels = rev(as.character(names_order)))) %&gt;%\n  ggplot(aes(x = index, y = value, colour = cluster_group)) +\n  geom_line() +\n  facet_wrap(~type_col, ncol = 1, strip.position = \"left\") +\n  guides(color = FALSE) +\n  theme_bw() +\n  theme(strip.background = element_blank(), strip.text = element_blank())\n\ngp1 &lt;- ggplotGrob(p1)\ngp2 &lt;- ggplotGrob(p2)\n\ngrid.arrange(gp2, gp1, ncol = 2, widths = c(4, 2))\n\n\n\n\n\n\n\n\n{TSclust} successfully separates the time series into two groups as expected. Looking closer, however, the ‘wall’ runs within their cluster aren’t perfectly ordered by shape. Let’s see if {dtwclust} performs better.\n\n\nUsing {dtwclust}\nThe standout feature of {dtwclust} is its high level of customization for the clustering process. The package vignette provides a comprehensive look at all the available options.\n\ncluster_dtw_h2 &lt;- dtwclust::tsclust(t(ts_sim_df),\n  type = \"h\",\n  k = 2,\n  distance = \"dtw\",\n  control = hierarchical_control(method = \"complete\"),\n  preproc = NULL,\n  args = tsclust_args(dist = list(window.size = 5L))\n)\n\nhclus &lt;- stats::cutree(cluster_dtw_h2, k = 2) %&gt;% # hclus &lt;- cluster::pam(dist_ts, k = 2)$clustering has a similar result\n  as.data.frame(.) %&gt;%\n  dplyr::rename(., cluster_group = .) %&gt;%\n  tibble::rownames_to_column(\"type_col\")\n\nhcdata &lt;- ggdendro::dendro_data(cluster_dtw_h2)\nnames_order &lt;- hcdata$labels$label\n# Use the folloing to remove labels from dendogram so not doubling up - but good for checking hcdata$labels$label &lt;- \"\"\n\np1 &lt;- hcdata %&gt;%\n  ggdendro::ggdendrogram(., rotate = TRUE, leaf_labels = FALSE)\n\np2 &lt;- ts_sim_df %&gt;%\n  dplyr::mutate(index = 1:420) %&gt;%\n  tidyr::gather(key = type_col, value = value, -index) %&gt;%\n  dplyr::full_join(., hclus, by = \"type_col\") %&gt;%\n  mutate(type_col = factor(type_col, levels = rev(as.character(names_order)))) %&gt;%\n  ggplot(aes(x = index, y = value, colour = cluster_group)) +\n  geom_line() +\n  facet_wrap(~type_col, ncol = 1, strip.position = \"left\") +\n  guides(color = FALSE) +\n  theme_bw() +\n  theme(strip.background = element_blank(), strip.text = element_blank())\n\ngp1 &lt;- ggplotGrob(p1)\ngp2 &lt;- ggplotGrob(p2)\n\ngrid.arrange(gp2, gp1, ncol = 2, widths = c(4, 2))\n\n\n\n\n\n\n\n\nThis result is better. The clusters correctly separate the ‘classic’ and ‘wall’ runs, and now, time series with similar shapes are also grouped together within each cluster.\nWe can refine this further by modifying the arguments to cluster based on z-scores and calculate centroids using the built-in shape_extraction() function.\n\ncluster_dtw_h2 &lt;- dtwclust::tsclust(t(ts_sim_df),\n  type = \"h\", k = 2L,\n  preproc = zscore,\n  distance = \"dtw\", centroid = shape_extraction,\n  control = hierarchical_control(method = \"complete\")\n)\n\nhclus &lt;- stats::cutree(cluster_dtw_h2, k = 2) %&gt;% # hclus &lt;- cluster::pam(dist_ts, k = 2)$clustering has a similar result\n  as.data.frame(.) %&gt;%\n  dplyr::rename(., cluster_group = .) %&gt;%\n  tibble::rownames_to_column(\"type_col\")\n\nhcdata &lt;- ggdendro::dendro_data(cluster_dtw_h2)\nnames_order &lt;- hcdata$labels$label\n# Use the folloing to remove labels from dendogram so not doubling up - but good for checking hcdata$labels$label &lt;- \"\"\n\np1 &lt;- hcdata %&gt;%\n  ggdendro::ggdendrogram(., rotate = TRUE, leaf_labels = FALSE)\n\np2 &lt;- ts_sim_df %&gt;%\n  dplyr::mutate(index = 1:420) %&gt;%\n  tidyr::gather(key = type_col, value = value, -index) %&gt;%\n  dplyr::full_join(., hclus, by = \"type_col\") %&gt;%\n  mutate(type_col = factor(type_col, levels = rev(as.character(names_order)))) %&gt;%\n  ggplot(aes(x = index, y = value, colour = cluster_group)) +\n  geom_line() +\n  facet_wrap(~type_col, ncol = 1, strip.position = \"left\") +\n  guides(color = FALSE) +\n  theme_bw() +\n  theme(strip.background = element_blank(), strip.text = element_blank())\n\ngp1 &lt;- ggplotGrob(p1)\ngp2 &lt;- ggplotGrob(p2)\n\ngrid.arrange(gp2, gp1, ncol = 2, widths = c(4, 2))\n\n\n\n\n\n\n\n\nAs shown in the vignette, we can also register a custom function for a normalized and asymmetric variant of DTW.\n\n# Normalized DTW\nndtw &lt;- function(x, y, ...) {\n  dtw(x, y, ...,\n    step.pattern = asymmetric,\n    distance.only = TRUE\n  )$normalizedDistance\n}\n# Register the distance with proxy\nproxy::pr_DB$set_entry(\n  FUN = ndtw, names = c(\"nDTW\"),\n  loop = TRUE, type = \"metric\", distance = TRUE,\n  description = \"Normalized, asymmetric DTW\"\n)\n# Partitional clustering\ncluster_dtw_h2 &lt;- dtwclust::tsclust(t(ts_sim_df), k = 2L, distance = \"nDTW\")\n\nplot(cluster_dtw_h2)\n\n\n\n\n\n\n\n\nWhile this partitional approach works well for the sine data, it’s less accurate for our ARIMA-based simulations. A drawback of this method is that I can’t extract a dendrogram from the cluster_dtw_h2 object directly, but the distance matrix it contains could still be useful.\nThis initial analysis shows the promise of DTW. To continue this work, future steps would involve testing the method on time series with greater dissimilarities and, most importantly, applying it to a real-world dataset."
  }
]